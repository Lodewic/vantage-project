geom_bar(stat = "identity") +
geom_text(aes(label=observations), position=position_dodge(width=0.9), vjust=-0.25) +
ggtitle("Aantal observaties per jaar") +
theme_fivethirtyeight()
# Barplot of number of observations per year
df_dates %>%
group_by(year_recorded) %>%
summarise(observations = n()) %>%
ggplot(aes(x = year_recorded, y = observations, fill = year_recorded)) +
geom_bar(stat = "identity") +
geom_text(aes(label=observations), position=position_dodge(width=0.9), vjust=-0.25) +
ggtitle("Aantal observaties per jaar") +
theme_fivethirtyeight()
library(reticulate)
install.packages("reticulate")
library(reticulate)
reticulate::use_condaenv("vantage-project", required = T)
reticulate::py_run_string("import dill")
# Hyper parameters
# Read data
df <- read.csv("../data/raw/water_pump_set.csv")
labels <- read.csv("../data/raw/water_pump_labels.csv")
feature_desc <- read.csv("../data/raw/water_pump_features.csv")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
# knitr::knit_engines$set(python = reticulate::eng_python)
# reticulate::use_condaenv("vantage-project", required = T)
py$a
library(reticulate)
py$a
# knitr::knit_engines$set(python = reticulate::eng_python)
reticulate::use_condaenv("vantage-project", required = F)
py$a
py_config()
?reticulate
conda_list()
?use_condaenv
use_condaenv("vantage-project", required = T)
reticulate::py_run_string("import pandas")
reticulate::py_run_string("b = 6")
py$b
py_run_string("b = 6")
# Load libraries
library(tidyverse)
library(broom)
library(ggthemes)
library(DT)
library(reticulate)
py_run_string("b = 6")
py$a
py$b
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
# knitr::knit_engines$set(python = reticulate::eng_python)
reticulate::use_condaenv("vantage-project", required = T)
# Load libraries
library(tidyverse)
library(broom)
library(ggthemes)
library(DT)
library(reticulate)
py_run_string("b = 6")
py$b
py$a
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
knitr::knit_engines$set(python = reticulate::eng_python)
library(reticulate)
reticulate::use_condaenv("vantage-project", required = F)
# Load libraries
library(tidyverse)
library(broom)
library(ggthemes)
library(DT)
py_run_string("a = 6")
# Load libraries
library(tidyverse)
library(broom)
library(ggthemes)
library(DT)
py_run_string("a = 6")
py_run_string('import umap')
py_run_string('import dill')
reticulate::use_condaenv("vantage-project", required = T)
py_run_string('import dill')
py_config()
reticulate::use_condaenv("vantage-project", required = T)
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
knitr::knit_engines$set(python = reticulate::eng_python)
library(reticulate)
reticulate::use_condaenv("vantage-project", required = T)
py_run_string('import dill')
py_config()
reticulate::use_condaenv("vantage-project", required = T)
py_available()
use_condaenv()
reticulate::use_condaenv("vantage-project", required = T)
py_config()
library(reticulate)
py_run_string("import dill")
Sys.getenv("LD_LIBRARY_PATH")
?py_run_string
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
knitr::knit_engines$set(python = reticulate::eng_python)
### NOTE ABOUT RETICULATE WITH CONDA ENVIRONMENTS ####
# You can NOT use non-activated environments with use_condaenv()!
#   Issue here https://github.com/rstudio/reticulate/issues/292 still open
# This means you need to start rstudio from within an activated environment
# i.e. on Unix
#   source activate vantage-project
#   rstudio vantage-project.Proj
# On windows CMD (not powershell)
#   conda env create -f environment.yml
#   activate vantage-project
#   "C:\Program Files\RStudio\bin\rstudio.exe"
# This will start RStudio using the Conda environment so that you can use the
# activated environment and the related Python version / packages
library(reticulate)
# reticulate::use_condaenv("vantage-project", required = F)
# Load libraries
library(tidyverse)
library(broom)
library(ggthemes)
library(DT)
py_run_string("a = 6")
py$a
py_config()
knitr::knit("temp_test.Rmd")
# This means you need to start rstudio from within an activated environment
# i.e. on Unix
#   source activate vantage-project
#   rstudio vantage-project.Proj
# On windows CMD (not powershell)
#   conda env create -f environment.yml
#   activate vantage-project
#   "C:\Program Files\RStudio\bin\rstudio.exe"
# This will start RStudio using the Conda environment so that you can use the
# activated environment and the related Python version / packages
library(reticulate)
>>>>>>> 4b8065222dbc969d8b10dea5275cde5979e19f65
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
knitr::knit_engines$set(python = reticulate::eng_python) # Not needed with knitr version > 1.18
library(reticulate)
### NOTE ABOUT RETICULATE WITH CONDA ENVIRONMENTS ####
# You can NOT use non-activated environments with use_condaenv()!
#   Issue here https://github.com/rstudio/reticulate/issues/292 still open
# This means you need to start rstudio from within an activated environment
# i.e. on Unix
#   source activate vantage-project
#   rstudio vantage-project.Proj
# On windows CMD (not powershell)
#   conda env create -f environment.yml
#   activate vantage-project
#   "C:\Program Files\RStudio\bin\rstudio.exe"
# This will start RStudio using the Conda environment so that you can use the
# activated environment and the related Python version / packages
# To render this document using the conda environment you can use,
#   R -e "rmarkdown::render('data-exploration.Rmd')"
use_condaenv('vantage-project', required = TRUE)    # Hopefully this issue is fixed and this will just work if the environment wasn't already activated
# use_python('D:/Anaconda/envs/vantage-project/python.exe')   # Alternatively this should work if you have an absolute path
# A major issue is that (on Windows) we can't use python chunks in combination with conda
# A workaround is to run all python code from external scripts or use reticulate::py_run_string()
feature_desc <- py$features %>%
mutate(example = as.character(py$data[1, -1]), # First row of the data without the 'id' column
unique = map_int(py$data[,-1],function(x) length(unique(x))),
type = map_chr(py$data[,-1], function(x) ifelse(is.numeric(x), "numeric", "character")))
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
knitr::knit_engines$set(python = reticulate::eng_python) # Not needed with knitr version > 1.18
library(reticulate)
### NOTE ABOUT RETICULATE WITH CONDA ENVIRONMENTS ####
# You can NOT use non-activated environments with use_condaenv()!
#   Issue here https://github.com/rstudio/reticulate/issues/292 still open
# This means you need to start rstudio from within an activated environment
# i.e. on Unix
#   source activate vantage-project
#   rstudio vantage-project.Proj
# On windows CMD (not powershell)
#   conda env create -f environment.yml
#   activate vantage-project
#   "C:\Program Files\RStudio\bin\rstudio.exe"
# This will start RStudio using the Conda environment so that you can use the
# activated environment and the related Python version / packages
# To render this document using the conda environment you can use,
#   R -e "rmarkdown::render('data-exploration.Rmd')"
use_condaenv('vantage-project', required = TRUE)    # Hopefully this issue is fixed and this will just work if the environment wasn't already activated
# use_python('D:/Anaconda/envs/vantage-project/python.exe')   # Alternatively this should work if you have an absolute path
# A major issue is that (on Windows) we can't use python chunks in combination with conda
# A workaround is to run all python code from external scripts or use reticulate::py_run_string()
# Load libraries
library(tidyverse)
library(broom)
library(ggthemes)
library(DT)
library(leaflet)
# PYTHON
# There are some issues with reticulate which causes R to crash when using
# python chunks in Rmarkdown with python from a conda environment
#   Might not be the case on Ubuntu / AWS servers but for now we use a workaround
#   with py_run_string()
py_run_string("
import sklearn as sk
import numpy as np
import pandas as pd
")
# Python code
# Read data from s3 amazon instance
py_run_string("
data = pd.read_csv('https://s3.amazonaws.com/drivendata/data/7/public/4910797b-ee55-40a7-8668-10efd5c1b960.csv', encoding = 'iso-8859-1')
labels = pd.read_csv('https://s3.amazonaws.com/drivendata/data/7/public/0bf8bc6e-30d0-4c50-956a-603fc693d966.csv', encoding = 'iso-8859-1')
# A .docx was provided with feature descriptions. This was formatted as a csv file.
features = pd.read_csv('../data/raw/water_pump_features.csv')
")
ShowDataTable <- function(df) {
datatable(df,
# filter = "top",
options = list(
scrollX = TRUE,
# autoWidth  = T,
paging = FALSE)
)
}
ShowDataTable(py$data[1:6,])
calc_groups_percentile <- function(x, p=0.8) {
if ((length(unique(x)) / length(x)) > 1-p) return(unique(x))
group_counts <- table(x) / length(x)
print(x[1])
# Number of groups needed to cover given quantile
group_cumsum <- cumsum(sort(group_counts, decreasing = T))
num_groups <- which(group_cumsum >= p)[1]
return(num_groups)
}
calc_groups_coverage <- function(x, maxgroups = 10) {
if (class(x) == "list") x <- unlist(x)
if (class(x) != "numeric") x <- as.character(x)
# print(class(x))
# print(length(unique(x)))
if (maxgroups >= length(unique(x))) return(1)
# else...
ratios = sort(table(x))[1:maxgroups] / length(x)
return(sum(ratios, na.rm=T))
}
feature_desc <- py$features %>%
mutate(example = as.character(py$data[1, -1]), # First row of the data without the 'id' column
unique = map_int(py$data[,-1],function(x) length(unique(x))),
type = map_chr(py$data[,-1], function(x) ifelse(is.numeric(x), "numeric", "character")))
# Number of existing groups required to cover at least 80% of the data
# group_req <- map(py$data, calc_groups_percentile, p=0.8)
# Coverage of data using only the largest groups
group_coverage <- map(py$data, calc_groups_coverage, maxgroups=10)
calc_groups_coverage <- function(x, maxgroups = 10) {
if (class(x) == "list") x <- unlist(x)
if (class(x) != "numeric") x <- as.character(x)
# print(class(x))
# print(length(unique(x)))
if (maxgroups >= length(unique(x))) return(1)
# else...
ratios = cumsum(sort(table(x))[1:maxgroups]) / length(x)
return(ratios[maxgroups])
}
calc_groups_coverage <- function(x, maxgroups = 10) {
if (class(x) == "list") x <- unlist(x)
if (class(x) != "numeric") x <- as.character(x)
# print(class(x))
# print(length(unique(x)))
if (maxgroups >= length(unique(x))) return(1)
# else...
ratios = cumsum(sort(table(x))[1:maxgroups]) / length(x)
return(ratios)
}
# Number of existing groups required to cover at least 80% of the data
# group_req <- map(py$data, calc_groups_percentile, p=0.8)
# Coverage of data using only the largest groups
group_coverage <- map(py$data, calc_groups_coverage, maxgroups=10)
View(group_coverage)
table(py$data$amount_tsh)
barplot(unlist(table(py$data$amount_tsh), use.names = F)
)
table(py$data$amount_tsh)[-1]
barplot(table(py$data$amount_tsh)[-1])
py$data %>% ggplot(aes(x = amount_tsh)) + geom_barplot()
py$data %>% ggplot(aes(x = amount_tsh)) + geom_bar()
py$data %>% ggplot(aes(x = factor(amount_tsh))) + geom_bar()
py$data %>% ggplot(aes(x = factor(amount_tsh))) + geom_bar()
calc_groups_coverage <- function(x, maxgroups = 10) {
if (class(x) == "list") x <- unlist(x)
if (class(x) != "numeric") x <- as.character(x)
# print(class(x))
# print(length(unique(x)))
if (maxgroups >= length(unique(x))) return(1)
# else...
ratios = cumsum(sort(table(x))[1:maxgroups]) / length(x)
return(ratios[1:maxgroups])
}
# Number of existing groups required to cover at least 80% of the data
# group_req <- map(py$data, calc_groups_percentile, p=0.8)
# Coverage of data using only the largest groups
group_coverage <- map(py$data, calc_groups_coverage, maxgroups=10)
data.frame(group_coverage)
data.frame(unlist(group_coverage))
data.frame(unlist(group_coverage, recursive = F))
unlist(group_coverage, recursive = F)
data.frame(group_coverage, recursive = F)
data.frame(group_coverage, recursive = F) %>% t()
df_group_coverage <- data.frame(group_coverage, recursive = F) %>% t()
df_group_coverage <- t(data.frame(group_coverage, recursive = F))
df_group_coverage <- t(data.frame(group_coverage))
View(df_group_coverage)
df_group_coverage <- data.frame(group_coverage)
View(df_group_coverage)
datatable(feature_desc %>%
filter(type == "numeric") %>%
arrange(unique))
knit_with_parameters('D:/GitHub/vantage-project/reports/feature-summary.Rmd')
knit_with_parameters('D:/GitHub/vantage-project/reports/feature-summary.Rmd')
knit_with_parameters('D:/GitHub/vantage-project/reports/feature-summary.Rmd')
knitr::opts_chunk$set(echo = TRUE)
df <- read.csv(params$data$filename)
View(params)
df <- read.csv(params$data)
View(params)
apply(df, 2, class)
df <- read.csv(params$data)
description <- read.csv(params$description)
feature_desc <- df %>%
mutate(example = as.character(py$data[1, -1]), # First row of the data without the 'id' column
unique = map_int(py$data[,-1],function(x) length(unique(x))),
type = map_chr(py$data[,-1], function(x) ifelse(is.numeric(x), "numeric", "character")))
df <- read.csv(params$data)
description <- read.csv(params$description)
feature_desc <- description %>%
mutate(example = as.character(df[1, -1]), # First row of the data without the 'id' column
unique = map_int(df[,-1],function(x) length(unique(x))),
type = map_chr(df[,-1], function(x) ifelse(is.numeric(x), "numeric", "character")))
View(feature_desc)
reticulate::repl_python()
random_state=0,
"""
=========================================
Feature importances with forests of trees
=========================================
This examples shows the use of forests of trees to evaluate the importance of
features on an artificial classification task. The red bars are the feature
importances of the forest, along with their inter-trees variability.
As expected, the plot suggests that 3 features are informative, while the
remaining are not.
"""
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.ensemble import ExtraTreesClassifier
# Build a classification task using 3 informative features
X, y = make_classification(n_samples=1000,
n_features=10,
n_informative=3,
n_redundant=0,
n_repeated=0,
n_classes=2,
random_state=0,
shuffle=False)
# Build a forest and compute the feature importances
forest = ExtraTreesClassifier(n_estimators=250,
random_state=0)
forest.fit(X, y)
importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_],
axis=0)
indices = np.argsort(importances)[::-1]
# Print the feature ranking
print("Feature ranking:")
for f in range(X.shape[1]):
print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))
# Plot the feature importances of the forest
plt.figure()
plt.title("Feature importances")
plt.bar(range(X.shape[1]), importances[indices],
color="r", yerr=std[indices], align="center")
plt.xticks(range(X.shape[1]), indices)
plt.xlim([-1, X.shape[1]])
plt.show()
print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))
"""
=========================================
Feature importances with forests of trees
=========================================
This examples shows the use of forests of trees to evaluate the importance of
features on an artificial classification task. The red bars are the feature
importances of the forest, along with their inter-trees variability.
As expected, the plot suggests that 3 features are informative, while the
remaining are not.
"""
print(__doc__)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.ensemble import ExtraTreesClassifier
# Build a classification task using 3 informative features
X, y = make_classification(n_samples=1000,
n_features=10,
n_informative=3,
n_redundant=0,
n_repeated=0,
n_classes=2,
random_state=0,
shuffle=False)
# Build a forest and compute the feature importances
forest = ExtraTreesClassifier(n_estimators=250,
random_state=0)
forest.fit(X, y)
importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_],
axis=0)
indices = np.argsort(importances)[::-1]
# Print the feature ranking
print("Feature ranking:")
for f in range(X.shape[1]):
print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))
# Plot the feature importances of the forest
plt.figure()
plt.title("Feature importances")
plt.bar(range(X.shape[1]), importances[indices],
color="r", yerr=std[indices], align="center")
plt.xticks(range(X.shape[1]), indices)
plt.xlim([-1, X.shape[1]])
plt.show()
X = pd.read_csv(args[X])
args = parser.parse_args()
from argparse import ArgumentParser
# Read arguments from command line
parser = ArgumentParser()
parser.add_argument("-f", "--file", dest="filename",
help="read input data from FILE", metavar="FILE")
parser.add_argument("-l", "--labels", dest="labelsfile",
help="don't print status messages to stdout")
args = parser.parse_args()
import pandas as pd
args
args[[filename]]
args['filename']
args[['filename']]
args('filename')
args(filename)
args
args[1]
args[[1]]
args(1)
args
args.filename
from argparse import ArgumentParser
# Read arguments from command line
parser = ArgumentParser()
parser.add_argument("-f", "--file", dest="filename", default="/data/raw/water_pumps_set,csv",
help="read input data from FILE", metavar="FILE")
parser.add_argument("-l", "--labels", dest="labelsfile", default="/data/raw/water_pumps_labels,csv",
help="don't print status messages to stdout")
args = parser.parse_args()
args
shuffle=False)
shuffle=False)
# Build a classification task using 3 informative features
X, y = make_classification(n_samples=1000,
n_features=10,
n_informative=3,
n_redundant=0,
n_repeated=0,
n_classes=2,
random_state=0,
shuffle=False)
X
y
y = pd.read_csv(args.labelsfile)[:, 2]
parser = ArgumentParser()
parser.add_argument("-f", "--file", dest="filename", default="../data/raw/water_pumps_set,csv",
help="read input data from FILE", metavar="FILE")
parser.add_argument("-l", "--labels", dest="labelsfile", default="../data/raw/water_pumps_labels,csv",
help="don't print status messages to stdout")
args = parser.parse_args()
y = pd.read_csv(args.labelsfile)[:, 2]
dir()
X = pd.read_csv(args.filename)
X.columns
df.columns
quit
View(df)
reticulate::repl_python()
help="read input data from FILE", metavar="FILE")
parser = ArgumentParser()
parser.add_argument("-f", "--file", dest="filename", default="D:/GitHub/vantage-project/data/raw/water_pumps_set,csv",
help="read input data from FILE", metavar="FILE")
parser.add_argument("-l", "--labels", dest="labelsfile", default="../data/raw/water_pumps_labels,csv",
help="don't print status messages to stdout")
args = parser.parse_args()
# Read data using pandas as simple as possible
X = pd.read_csv(args.filename)
parser = ArgumentParser()
parser.add_argument("-f", "--file", dest="filename", default="D:/GitHub/vantage-project/data/raw/water_pump_set.csv",
help="read input data from FILE", metavar="FILE")
parser.add_argument("-l", "--labels", dest="labelsfile", default="../data/raw/water_pumps_labels,csv",
help="don't print status messages to stdout")
args = parser.parse_args()
# Read data using pandas as simple as possible
X = pd.read_csv(args.filename)
help="don't print status messages to stdout")
# https://stackoverflow.com/questions/1009860/how-to-read-process-command-line-arguments :)
parser = ArgumentParser()
parser.add_argument("-f", "--file", dest="filename", default="D:/GitHub/vantage-project/data/raw/water_pump_set.csv",
help="read input data from FILE", metavar="FILE")
parser.add_argument("-l", "--labels", dest="labelsfile", default="../data/raw/water_pumps_labels.csv",
help="don't print status messages to stdout")
args = parser.parse_args()
# Read data using pandas as simple as possible
X = pd.read_csv(args.filename)
y = pd.read_csv(args.labelsfile)[:, 2]
df['status_group'] = pd.merge(df, ydat)
df.dtypes
