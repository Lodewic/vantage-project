---
title: "Data exploration"
author: "Lodewic van Twillert"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    fig_caption: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
params:
  echo:
    input: checkbox
    value: no
  project.name:
    input: character
    label: Short name used for this project
    value: Exploratory analysis
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)

knitr::knit_engines$set(python = reticulate::eng_python) # Not needed with knitr version > 1.18

library(reticulate)
### NOTE ABOUT RETICULATE WITH CONDA ENVIRONMENTS ####
# You can NOT use non-activated environments with use_condaenv()!
#   Issue here https://github.com/rstudio/reticulate/issues/292 still open
# This means you need to start rstudio from within an activated environment
# i.e. on Unix
#   source activate vantage-project
#   rstudio vantage-project.Proj
# On windows CMD (not powershell)
#   conda env create -f environment.yml
#   activate vantage-project
#   "C:\Program Files\RStudio\bin\rstudio.exe"
# This will start RStudio using the Conda environment so that you can use the 
# activated environment and the related Python version / packages
# To render this document using the conda environment you can use,
#   R -e "rmarkdown::render('data-exploration.Rmd')"

use_condaenv('vantage-project', required = TRUE)    # Hopefully this issue is fixed and this will just work if the environment wasn't already activated
# use_python('D:/Anaconda/envs/vantage-project/python.exe')   # Alternatively this should work if you have an absolute path

# A major issue is that (on Windows) we can't use python chunks in combination with conda
# A workaround is to run all python code from external scripts or use reticulate::py_run_string()
```

```{r libraries, include=FALSE, message=FALSE, warning=FALSE}
# Load libraries
library(tidyverse)
library(broom)
library(ggthemes)
library(DT)
library(knitr)
```

```{r Python packages}
# PYTHON

# There are some issues with reticulate which causes R to crash when using
# python chunks in Rmarkdown with python from a conda environment
#   Might not be the case on Ubuntu / AWS servers but for now we use a workaround
#   with py_run_string()

py_run_string("
import sklearn as sk
import numpy as np
import pandas as pd
")
```

```{r Read data}
data <- read.csv("../data/raw/water_pump_set.csv")
labels <- read.csv("../data/raw/water_pump_labels.csv")
features <- read.csv("../data/raw/water_pump_features.csv")
df <- merge(data, labels, by="id")
```

```{r Temporary included convenience functions, include=FALSE}
ShowDataTable <- function(df) {
  datatable(df,
    # filter = "top",
    options = list(
      scrollX = TRUE,
      # autoWidth  = T,
      paging = FALSE)
  )
}
```

# Getting a feeling for the data

Before we start modelling the data we should understand the data. In this dataset
we find a total of 40 measured variables to be used as features in our models. In this
section we will aim to understand these features and make motivated choices to keep, remove or transform these
features from our prediction model.

## A closer look at our features

Our data consists of,
  - `r nrow(df)` observations 
  - `r ncol(data)` features
  - `r length(unique(labels[,2]))` types of labels to predict
  
Not all of the features are necessarily useful for predicting the moment of failure of water pumps, 
for instance the measurement `id` won't be of much use at it is unique.

First we'll take a look at the raw data to fully understand the structure of our data,
especially how exactly the data was measured over time. We have `r nrow(df)` observations.

### The raw data

The raw data is not considered big-data in the sense that it will fit on a single machine; we do not have to worry about batch processing because of sheer file size with 'only' `r nrow(df)` observations. The number of features is also a mere `r ncol(df)`! Enough to make a interpretable predictive model and this will allow us to dive deeper into the
individual variables.

```{r}
head(df, n=10) %>% ShowDataTable()
```

With the data above we will aim to predict the labels, indicating the need of repair
or rather the functional state of water pumps. There are `r length(unique(labels[,2]))`  possible labels,

`r for (lab in unique(labels[,2])) cat(paste0("\t- ", lab, "(", round(sum(labels[,2] == lab) / nrow(labels), 2), "%)\n"))`

Each of these labels is associated with a unique measurement `id` that we can match to a single observation.

### The labels

The label distribution is heavily skewed, the 'functional needs repair' group
is severely under represented.
```{r}
df %>% ggplot(aes(x = status_group, fill = status_group)) + geom_bar() +
  ggtitle("Distribution of maintenance labels") +
  geom_bar() +
  theme_fivethirtyeight() 
```

### The surprise 

The water pump data is surprising in that it is not really a timeseries that you would
normally see in similar cases where the objective is to predict system failure using
sensor data over time. In the data we'll find a feature named `wpt_name`, short for waterpoint name, a name
that is intuitively unique for every water pump. However... When we look at the non-numerical
variables we see that many of the labels have a large number of unique values, as expected, but these also include `wpt_name`! 


